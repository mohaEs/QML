{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15601,
     "status": "ok",
     "timestamp": 1744238282907,
     "user": {
      "displayName": "Zayan Hasan",
      "userId": "03259132731488643797"
     },
     "user_tz": 240
    },
    "id": "fiTUY0nflfrm",
    "outputId": "664f6005-0361-4d91-ef29-afd450052393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1744238282939,
     "user": {
      "displayName": "Zayan Hasan",
      "userId": "03259132731488643797"
     },
     "user_tz": 240
    },
    "id": "Oq0bQizzm1nN"
   },
   "outputs": [],
   "source": [
    "results_dir = \"/content/drive/MyDrive/H/qmlData\"\n",
    "data_dir = \"/content/drive/MyDrive/H/qmlData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 21117,
     "status": "ok",
     "timestamp": 1744238304058,
     "user": {
      "displayName": "Zayan Hasan",
      "userId": "03259132731488643797"
     },
     "user_tz": 240
    },
    "id": "Sbus_Ofwmai-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.colab import drive\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import random\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1744238304716,
     "user": {
      "displayName": "Zayan Hasan",
      "userId": "03259132731488643797"
     },
     "user_tz": 240
    },
    "id": "kTXCk5Y1ma5E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1744238853192,
     "user": {
      "displayName": "Zayan Hasan",
      "userId": "03259132731488643797"
     },
     "user_tz": 240
    },
    "id": "Zf1Jg4MNmzfw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n",
    "def load_all_results(data_dir=\"qmlData\"):\n",
    "    all_results = {}\n",
    "\n",
    "    pkl_files = [f for f in os.listdir(data_dir) if f.endswith(\".pkl\")]\n",
    "\n",
    "    for pkl_file in pkl_files:\n",
    "        file_parts = pkl_file.split('_')\n",
    "\n",
    "        model = file_parts[0]\n",
    "\n",
    "        dataset = '_'.join(file_parts[1:-1])\n",
    "\n",
    "        file_path = os.path.join(data_dir, pkl_file)\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "\n",
    "            if dataset not in all_results:\n",
    "                all_results[dataset] = {}\n",
    "\n",
    "            all_results[dataset][model] = results\n",
    "            print(f\"Loaded: {model} model for {dataset} dataset\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def compare_accuracy_bar_plots(all_results, data_dir=\"qmlData\", compare_pairs=[(\"svm\", \"qsvm\"), (\"knn\", \"qknn\")]):\n",
    "    for model1, model2 in compare_pairs:\n",
    "        datasets = []\n",
    "        acc1 = []\n",
    "        acc2 = []\n",
    "\n",
    "        for dataset, models in all_results.items():\n",
    "            if model1 in models and model2 in models:\n",
    "                datasets.append(dataset)\n",
    "                acc1.append(models[model1]['accuracy'])\n",
    "                acc2.append(models[model2]['accuracy'])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(datasets))\n",
    "\n",
    "        color1 = 'steelblue'  # Classical methods\n",
    "        color2 = 'lightblue'  # Quantum methods\n",
    "\n",
    "        plt.bar(index, acc1, bar_width, label=model1.upper(), color=color1)\n",
    "        plt.bar(index + bar_width, acc2, bar_width, label=model2.upper(), color=color2)\n",
    "\n",
    "        plt.xlabel('Dataset', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title(f'Accuracy Comparison: {model1.upper()} vs {model2.upper()}', fontsize=14)\n",
    "        plt.xticks(index + bar_width/2, datasets, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "\n",
    "        for i, v in enumerate(acc1):\n",
    "            plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "        for i, v in enumerate(acc2):\n",
    "            plt.text(i + bar_width, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(data_dir, f'accuracy_comparison_{model1}_vs_{model2}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def calculate_auc_values(all_results):\n",
    "    auc_values = {}\n",
    "\n",
    "    for dataset, models in all_results.items():\n",
    "        auc_values[dataset] = {}\n",
    "\n",
    "        for model_name, results in models.items():\n",
    "            y_test = results['y_test']\n",
    "            y_prob = results['y_prob']\n",
    "\n",
    "            n_classes = len(np.unique(y_test))\n",
    "\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_prob[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            mean_tpr /= n_classes\n",
    "            macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "            auc_values[dataset][model_name] = macro_auc\n",
    "\n",
    "    return auc_values\n",
    "\n",
    "def compare_auc_bar_plots(all_results, data_dir=\"qmlData\", compare_pairs=[(\"svm\", \"qsvm\"), (\"knn\", \"qknn\")]):\n",
    "    auc_values = calculate_auc_values(all_results)\n",
    "\n",
    "    for model1, model2 in compare_pairs:\n",
    "        datasets = []\n",
    "        dataset_labels = ['APTOS-2019 (Diabetic Retinopathy)', 'MESSIDOR (Diabetic Retinopathy)', 'IDRID (Diabetic Retinopathy)', 'PAPILA (Glaucoma)', 'Glaucoma Fundus (Glaucoma)', 'G1020 (Glaucoma)']\n",
    "        auc1 = []\n",
    "        auc2 = []\n",
    "\n",
    "        for dataset, models in auc_values.items():\n",
    "            if model1 in models and model2 in models:\n",
    "                datasets.append(dataset)\n",
    "                auc1.append(models[model1])\n",
    "                auc2.append(models[model2])\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(len(datasets))\n",
    "\n",
    "        color1 = 'steelblue'  # Classical methods\n",
    "        color2 = 'lightblue'  # Quantum methods\n",
    "\n",
    "        plt.bar(index, auc1, bar_width, label=model1.upper(), color=color1, alpha=0.7)\n",
    "        plt.bar(index + bar_width, auc2, bar_width, label=model2.upper(), color=color2, alpha=0.7)\n",
    "\n",
    "        plt.xlabel('Dataset', fontsize=12)\n",
    "        plt.ylabel('AUC', fontsize=12)\n",
    "        plt.title(f'AUC Comparison: {model1.upper()} vs {model2.upper()}', fontsize=14)\n",
    "        plt.xticks(index + bar_width/2, dataset_labels, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "\n",
    "        for i, v in enumerate(auc1):\n",
    "            plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "        for i, v in enumerate(auc2):\n",
    "            plt.text(i + bar_width, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(data_dir, f'auroc_comparison_{model1}_vs_{model2}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        mean_auc1 = np.mean(auc1)\n",
    "        mean_auc2 = np.mean(auc2)\n",
    "\n",
    "        plt.figure(figsize=(6, 8))\n",
    "        plt.bar(0, mean_auc1, bar_width*2, label=model1.upper(), color=color1, alpha = 0.7)\n",
    "        plt.bar(1, mean_auc2, bar_width*2, label=model2.upper(), color=color2, alpha = 0.7)\n",
    "\n",
    "        plt.ylabel('Mean AUC', fontsize=12)\n",
    "        plt.title(f'Mean AUC Across Datasets: {model1.upper()} vs {model2.upper()}', fontsize=14)\n",
    "        plt.xticks([0, 1], [model1.upper(), model2.upper()])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.text(0, mean_auc1 + 0.01, f'{mean_auc1:.3f}', ha='center')\n",
    "        plt.text(1, mean_auc2 + 0.01, f'{mean_auc2:.3f}', ha='center')\n",
    "\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(data_dir, f'mean_auc_comparison_{model1}_vs_{model2}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curves(all_results, data_dir=\"qmlData\"):\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    for dataset, models in all_results.items():\n",
    "        plt.figure(figsize=(8, 7))\n",
    "\n",
    "        first_model = next(iter(models.values()))\n",
    "        class_names = first_model['class_names']\n",
    "        n_classes = len(class_names)\n",
    "\n",
    "        colors = cycle(['blue', 'green', 'orange', 'red'])\n",
    "        line_styles = cycle(['-', '--', ':', '-.'])\n",
    "\n",
    "        for model_name, results in models.items():\n",
    "            y_test = results['y_test']\n",
    "            y_prob = results['y_prob']\n",
    "\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            for i in range(n_classes):\n",
    "                fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_prob[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(n_classes):\n",
    "                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "            color = next(colors)\n",
    "            linestyle = next(line_styles)\n",
    "            plt.plot(\n",
    "                all_fpr, mean_tpr,\n",
    "                label=f'{model_name.upper()} (AUC = {macro_auc:.2f})',\n",
    "                color=color, linestyle=linestyle, linewidth=2\n",
    "            )\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.xlabel('False Positive Rate', fontsize=18)\n",
    "        plt.ylabel('True Positive Rate', fontsize=18)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.title(f'ROC Curves for {dataset} Dataset', fontsize=22)\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(data_dir, f'a_roc_curves_{dataset}.pdf'), format='pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        for model_name, results in models.items():\n",
    "            plt.figure(figsize=(8, 7))\n",
    "\n",
    "            y_test = results['y_test']\n",
    "            y_prob = results['y_prob']\n",
    "\n",
    "            for i, class_name in enumerate(class_names):\n",
    "                fpr, tpr, _ = roc_curve((y_test == i).astype(int), y_prob[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "\n",
    "                plt.plot(\n",
    "                    fpr, tpr,\n",
    "                    label=f'Class {class_name} (AUC = {roc_auc:.2f})',\n",
    "                    linewidth=2\n",
    "                )\n",
    "\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            for i in range(n_classes):\n",
    "                fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_prob[:, i])\n",
    "\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(n_classes):\n",
    "                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "            plt.plot(\n",
    "                all_fpr, mean_tpr,\n",
    "                label=f'Macro-average (AUC = {macro_auc:.2f})',\n",
    "                color='navy', linestyle=':', linewidth=4\n",
    "            )\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.0])\n",
    "            plt.xlabel('False Positive Rate', fontsize=18)\n",
    "            plt.ylabel('True Positive Rate', fontsize=18)\n",
    "            plt.xticks(fontsize=16)\n",
    "            plt.yticks(fontsize=16)\n",
    "            plt.title(f'ROC Curves: {model_name.upper()} on {dataset}', fontsize=22)\n",
    "            plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(data_dir, f'a_roc_curves_{model_name}_{dataset}.pdf'), format='pdf', bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def create_performance_tables(all_results, data_dir=\"qmlData\"):\n",
    "    overall_metrics = []\n",
    "\n",
    "    for dataset, models in all_results.items():\n",
    "        for model_name, results in models.items():\n",
    "            overall_metrics.append({\n",
    "                'Dataset': dataset,\n",
    "                'Model': model_name.upper(),\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall'],\n",
    "                'F1 Score': results['f1'],\n",
    "                'Is Quantum': 'Yes' if results['is_quantum'] else 'No'\n",
    "            })\n",
    "\n",
    "    metrics_df = pd.DataFrame(overall_metrics)\n",
    "\n",
    "    metrics_df = metrics_df.sort_values(['Dataset', 'Model'])\n",
    "\n",
    "    print(\"\\n===== OVERALL PERFORMANCE METRICS =====\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "    metrics_df.to_csv(os.path.join(data_dir, 'performance_metrics.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1sVLIDEkkqjQpfaqwKmSK0rEwjlZ9FMK4"
    },
    "executionInfo": {
     "elapsed": 14937,
     "status": "ok",
     "timestamp": 1744238869513,
     "user": {
      "displayName": "Zayan Hasan",
      "userId": "03259132731488643797"
     },
     "user_tz": 240
    },
    "id": "fSA-3waS1f8I",
    "outputId": "9a247a42-4bce-4c93-ebcb-2222c3af606a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = data_dir\n",
    "results = load_all_results(data_dir)\n",
    "plot_roc_curves(results, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_iBHsLy3pV6"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_dir = data_dir\n",
    "    results = load_all_results(data_dir)\n",
    "    compare_accuracy_bar_plots(results, data_dir)\n",
    "    compare_auc_bar_plots(results, data_dir)\n",
    "    plot_roc_curves(results, data_dir)\n",
    "    create_performance_tables(results, data_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVyvoZS5vGXqA/KnjviZUt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
